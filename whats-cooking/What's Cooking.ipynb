{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For NN:\n",
    "\n",
    "GPU run mode:\n",
    "\n",
    "```\n",
    "THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 ipython notebook\n",
    "```\n",
    "\n",
    "CPU run mode:\n",
    "\n",
    "```\n",
    "THEANO_FLAGS=mode=FAST_RUN,device=cpu,floatX=float32 ipython notebook\n",
    "```\n",
    "\n",
    "To use Theano you need NVIDIA GPUs, hence won't work on some Macbook Pros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(10000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 10 seconds\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GRID K520 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "%autosave 10\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import model_from_json\n",
    "from keras.regularizers import l1l2, activity_l1l2\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import History, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## important, check\n",
    "\n",
    "you must see `Using gpu device 0: GRID K520 (CNMeM is disabled)` above, or else you're not using the GPU for NN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume I forgot to package a certain Python module and I desperately want it without SSH'ing to the master. I can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata: ....\n",
      "Solving package specifications: .....................\n",
      "# All requested packages already installed.\n",
      "# packages in environment at /home/ubuntu/miniconda:\n",
      "#\n",
      "nltk                      3.1                      py27_0  \n"
     ]
    }
   ],
   "source": [
    "!/home/ubuntu/miniconda/bin/conda install -y nltk\n",
    "\n",
    "# if not found by conda, use pip\n",
    "# !/home/ubuntu/miniconda/bin/pip install my-module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_filepath = \"/mnt/kaggle-whats-cooking/train.json\"\n",
    "all_train = pd.read_json(all_train_filepath)\n",
    "X_all_train = all_train[\"ingredients\"]\n",
    "y_all_train = all_train[\"cuisine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cuisine_encoder = LabelEncoder()\n",
    "cuisine_encoder.fit(y_all_train)\n",
    "y_all_train_encoded = cuisine_encoder.transform(y_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all_train, y_all_train_encoded, train_size=0.8, random_state=42,\n",
    "    stratify=y_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def process_ingredients(x):\n",
    "    \"\"\"scikit-learn requires a list of strings as the output of the\n",
    "    tokenizer. However we already start with a list of strings\n",
    "    (normally problems start with a plain string).\"\"\"\n",
    "    all_ingredients = []\n",
    "    for i in xrange(len(x)):\n",
    "        x[i] = re.sub('\\([^)]+\\)\\s*', '', x[i]) #  remove brackets\n",
    "        x[i] = re.sub('[^\\\\%]+\\\\%\\s*', '', x[i]) #  remove percent signs\n",
    "        x[i] = x[i].strip().lower()  # lower case, trim\n",
    "        x[i] = stemmer.stem(x[i])  #  reduce ingredient to its stem, minor accuracy improvement\n",
    "        result = [x[i]] + x[i].split(' ')\n",
    "        for j in xrange(len(result)):\n",
    "            result[j] = result[j].strip().lower()\n",
    "            result[j] = re.sub('[^a-z]', ' ', result[j])\n",
    "        for ingredient in result:\n",
    "            if len(ingredient.strip()) != 0:\n",
    "                all_ingredients.append(ingredient)\n",
    "    return all_ingredients\n",
    "\n",
    "X_train_vectorizer = CountVectorizer(min_df=1, binary=True,\n",
    "                                     tokenizer=process_ingredients,\n",
    "                                     lowercase=False,\n",
    "                                     ngram_range=(1, 2),\n",
    "                                     dtype=np.uint8)\n",
    "\n",
    "# does significantly worse than just using counts\n",
    "#X_train_vectorizer = TfidfVectorizer(min_df=1, binary=True,\n",
    "#                                     tokenizer=process_ingredients,\n",
    "#                                     lowercase=False,\n",
    "#                                     ngram_range=(1, 2))\n",
    "\n",
    "# we have around 1 million features. you can use the hashing trick to\n",
    "# vectorize to a smaller number of features without losing too much\n",
    "# information. could be an interesting step to take before deep\n",
    "# neural network trainining (reduce the number of features to allow\n",
    "# larger and more complex models to fit in the GPU's memory). however\n",
    "# this is pointless for SGDClassifier (i.e. logistic regression) because\n",
    "# we can fit the counts as a sparse matrix in memory, and we don't mind\n",
    "# how long it takes to train the model.\n",
    "#X_train_vectorizer = HashingVectorizer(binary=True,\n",
    "#                                       tokenizer=process_ingredients,\n",
    "#                                       lowercase=False,\n",
    "#                                       ngram_range=(1, 2),\n",
    "#                                       n_features=500000)\n",
    "\n",
    "X_train_tf = X_train_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[7 9 ..., 1 7], n_iter=10, test_size=0.1, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,\n",
       "       penalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0,\n",
       "       warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['elasticnet'], 'alpha': [0.0001], 'n_iter': [5], 'loss': ['log'], 'l1_ratio': array([ 0.     ,  0.05556,  0.11111,  0.16667,  0.22222,  0.27778,\n",
       "        0.33333,  0.38889,  0.44444,  0.5    ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'alpha': [0.0001],  # regularization, penalizes large coefficients\n",
    "    'l1_ratio': np.linspace(0, 0.5, num=10),  # elastic net param. 0 is L2, 1 is L1\n",
    "    'n_iter': [5],\n",
    "    'penalty': ['elasticnet'],\n",
    "    'loss': ['log'],\n",
    "}\n",
    "model = SGDClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "cv = StratifiedShuffleSplit(y_train, n_iter=10, test_size=0.1,\n",
    "                            random_state=42)\n",
    "grid = GridSearchCV(model, param_grid=param_grid, cv=cv)\n",
    "grid.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001,\n",
       " 'l1_ratio': 0.1111111111111111,\n",
       " 'loss': 'log',\n",
       " 'n_iter': 5,\n",
       " 'penalty': 'elasticnet'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_tf = X_train_vectorizer.transform(X_test)\n",
    "y_test_pred = grid.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778280542986\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(y_test, y_test_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_filepath = \"/mnt/kaggle-whats-cooking/test.json\"\n",
    "submission = pd.read_json(submission_filepath)\n",
    "X_submission = submission[\"ingredients\"]\n",
    "X_submission_df = X_train_vectorizer.transform(X_submission)\n",
    "y_submission = grid.predict(X_submission_df)\n",
    "y_cuisine = cuisine_encoder.inverse_transform(y_submission)\n",
    "df = pd.DataFrame({\"cuisine\": y_cuisine}, index=submission[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See:\n",
    "\n",
    "- http://nbviewer.ipython.org/github/Egor-Krivov/cooking/blob/master/kaggle.ipynb\n",
    "- https://github.com/Egor-Krivov/cooking/blob/master/utils.py\n",
    "- https://github.com/fchollet/keras/blob/master/examples/kaggle_otto_nn.py\n",
    "- http://cs231n.github.io/\n",
    "\n",
    "Papers:\n",
    "\n",
    "- http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a way to check if you're successfully using the GPU, or falling back to the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GpuElemwise{exp,no_inplace}(<CudaNdarrayType(float32, vector)>), HostFromGpu(GpuElemwise{exp,no_inplace}.0)]\n",
      "Looping 1000 times took 0.589230 seconds\n",
      "Result is [ 1.23178029  1.61879349  1.52278066 ...,  2.20771813  2.29967761\n",
      "  1.62323296]\n",
      "Used the gpu\n"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, sandbox\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], T.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in xrange(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_hot_encoded = np.array(\n",
    "    np.zeros((y_train.shape[0], len(cuisine_encoder.classes_))),\n",
    "    dtype=np.int8)\n",
    "for i, value in enumerate(y_train):\n",
    "    y_train_hot_encoded[i, value] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compile the model and load it onto the GPU. This takes a few minutes. If you get a memory error restart the kernel and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dropout = 0.75\n",
    "dropout = 0.75\n",
    "layers = [1024, 1024]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dropout(input_dropout, input_shape=(X_train_tf.shape[1], )))  # regularize inputs\n",
    "for layer in layers:\n",
    "    model.add(Dense(layer))\n",
    "\n",
    "    # choose an activation\n",
    "    model.add(Activation('relu'))  # the default activation to try first\n",
    "    #model.add(PReLU())\n",
    "\n",
    "    #model.add(BatchNormalization())  # makes it worse\n",
    "    model.add(Dropout(dropout))  # regularization, 0.5 is a good default dropout rate\n",
    "model.add(Dense(y_train_hot_encoded.shape[1]))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# optimizers. adam is much faster, sgd is more optimal (i think)\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = Adam()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "model_description = model.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we train. You can still abort the notebook here, but you'll probably have to restart the kernel and reload the data. Note that when you want to re-run this if you get memory errors you need to restart the kernel and reload the data.\n",
    "\n",
    "Also note that if you interrupt the kernel and restart training it doesn't start from scratch. Restarting the kernel will of course start everything from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28636 samples, validate on 3182 samples\n",
      "Epoch 1/100\n",
      "28636/28636 [==============================] - 24s - loss: 2.6675 - acc: 0.2168 - val_loss: 2.2927 - val_acc: 0.3953\n",
      "Epoch 2/100\n",
      "28636/28636 [==============================] - 24s - loss: 2.2078 - acc: 0.3869 - val_loss: 1.8157 - val_acc: 0.5104\n",
      "Epoch 3/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.9006 - acc: 0.4704 - val_loss: 1.5300 - val_acc: 0.5550\n",
      "Epoch 4/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.7198 - acc: 0.5031 - val_loss: 1.3748 - val_acc: 0.5968\n",
      "Epoch 5/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.6111 - acc: 0.5326 - val_loss: 1.2795 - val_acc: 0.6153\n",
      "Epoch 6/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.5357 - acc: 0.5472 - val_loss: 1.2165 - val_acc: 0.6439\n",
      "Epoch 7/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.4845 - acc: 0.5644 - val_loss: 1.1712 - val_acc: 0.6618\n",
      "Epoch 8/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.4464 - acc: 0.5718 - val_loss: 1.1379 - val_acc: 0.6697\n",
      "Epoch 9/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.4053 - acc: 0.5836 - val_loss: 1.1032 - val_acc: 0.6763\n",
      "Epoch 10/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.3716 - acc: 0.5927 - val_loss: 1.0808 - val_acc: 0.6942\n",
      "Epoch 11/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.3531 - acc: 0.5986 - val_loss: 1.0622 - val_acc: 0.7030\n",
      "Epoch 12/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.3338 - acc: 0.6041 - val_loss: 1.0373 - val_acc: 0.7055\n",
      "Epoch 13/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.3164 - acc: 0.6099 - val_loss: 1.0205 - val_acc: 0.7096\n",
      "Epoch 14/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.3058 - acc: 0.6139 - val_loss: 1.0095 - val_acc: 0.7165\n",
      "Epoch 15/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.2751 - acc: 0.6207 - val_loss: 0.9943 - val_acc: 0.7172\n",
      "Epoch 16/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.2621 - acc: 0.6254 - val_loss: 0.9867 - val_acc: 0.7172\n",
      "Epoch 17/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.2529 - acc: 0.6252 - val_loss: 0.9785 - val_acc: 0.7253\n",
      "Epoch 18/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.2364 - acc: 0.6290 - val_loss: 0.9648 - val_acc: 0.7307\n",
      "Epoch 19/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.2196 - acc: 0.6336 - val_loss: 0.9532 - val_acc: 0.7319\n",
      "Epoch 20/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.2063 - acc: 0.6373 - val_loss: 0.9408 - val_acc: 0.7382\n",
      "Epoch 21/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.2017 - acc: 0.6390 - val_loss: 0.9395 - val_acc: 0.7366\n",
      "Epoch 22/100\n",
      "28636/28636 [==============================] - 24s - loss: 1.1864 - acc: 0.6429 - val_loss: 0.9346 - val_acc: 0.7414\n",
      "Epoch 23/100\n",
      " 6144/28636 [=====>........................] - ETA: 17s - loss: 1.1681 - acc: 0.6496"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-4c1baa811a06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mshow_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m )\n",
      "\u001b[1;32m/home/ubuntu/miniconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, show_accuracy, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m    579\u001b[0m                          \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m                          \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                          shuffle=shuffle, metrics=metrics)\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/miniconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, metrics)\u001b[0m\n\u001b[0;32m    237\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/miniconda/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/miniconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 2048\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_tf.todense(),\n",
    "    y_train_hot_encoded,\n",
    "    nb_epoch=nb_epoch,\n",
    "    batch_size=batch_size,\n",
    "    show_accuracy=True,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw history of accuracies during training to see if we're overfitting or underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EpochDrawer(object):\n",
    "    '''\n",
    "    takes the history of keras.models.Sequential().fit() and\n",
    "    plots training and validation accuracy over the epochs\n",
    "    '''\n",
    "    def __init__(self, history, key='acc', save_filename = None):\n",
    "\n",
    "        self.x = history.epoch\n",
    "        self.legend = [key]\n",
    "\n",
    "        plt.plot(self.x, history.history[key], marker='.')\n",
    "\n",
    "        if 'val_%s' % key in history.history:\n",
    "            self.legend.append('val %s' % key)\n",
    "            plt.plot(self.x, history.history['val_%s' % key], marker='.')\n",
    "\n",
    "        plt.title('%s over epochs' % key)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.xticks(history.epoch, history.epoch)\n",
    "        plt.legend(self.legend, loc = 'upper right')\n",
    "\n",
    "        if save_filename is not None:\n",
    "            plt.savefig(save_filename)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EpochDrawer(history)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember - the smaller the gap between the curves the higher the bias. Small gap => try more complex, big gap => you need to regularize more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_tf = X_train_vectorizer.transform(X_test)\n",
    "y_test_pred = model.predict_classes(X_test_tf.todense(), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = accuracy_score(y_test, y_test_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO - ensemble NN and logistic regression?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
