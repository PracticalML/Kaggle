{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For NN:\n",
    "\n",
    "GPU run mode:\n",
    "\n",
    "```\n",
    "THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 ipython notebook\n",
    "```\n",
    "\n",
    "CPU run mode:\n",
    "\n",
    "```\n",
    "THEANO_FLAGS=mode=FAST_RUN,device=cpu,floatX=float32 ipython notebook\n",
    "```\n",
    "\n",
    "To use Theano you need NVIDIA GPUs, hence won't work on some Macbook Pros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(10000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 10 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 10\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import model_from_json\n",
    "from keras.regularizers import l2, l1l2, activity_l1l2\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import History, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## important, check\n",
    "\n",
    "you must see `Using gpu device 0: GRID K520 (CNMeM is disabled)` above, or else you're not using the GPU for NN training. the first time you run theano it may take 1-2 minutes to see this message, be patient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume I forgot to package a certain Python module and I desperately want it without SSH'ing to the master. I can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata: ....\n",
      "Solving package specifications: .....................\n",
      "Package plan for installation in environment /home/ubuntu/miniconda:\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    nltk-3.1                   |           py27_0         1.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    nltk: 3.1-py27_0\n",
      "\n",
      "Fetching packages ...\n",
      "nltk-3.1-py27_ 100% |################################| Time: 0:00:01   1.41 MB/s\n",
      "Extracting packages ...\n",
      "[      COMPLETE      ]|###################################################| 100%\n",
      "Linking packages ...\n",
      "[      COMPLETE      ]|###################################################| 100%\n"
     ]
    }
   ],
   "source": [
    "!/home/ubuntu/miniconda/bin/conda install -y nltk\n",
    "\n",
    "# if not found by conda, use pip\n",
    "# !/home/ubuntu/miniconda/bin/pip install my-module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_filepath = \"/mnt/kaggle-whats-cooking/train.json\"\n",
    "all_train = pd.read_json(all_train_filepath)\n",
    "X_all_train = all_train[\"ingredients\"]\n",
    "y_all_train = all_train[\"cuisine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cuisine_encoder = LabelEncoder()\n",
    "cuisine_encoder.fit(y_all_train)\n",
    "y_all_train_encoded = cuisine_encoder.transform(y_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all_train, y_all_train_encoded, train_size=0.8, random_state=42,\n",
    "    stratify=y_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def process_ingredients(x):\n",
    "    \"\"\"scikit-learn requires a list of strings as the output of the\n",
    "    tokenizer. However we already start with a list of strings\n",
    "    (normally problems start with a plain string).\"\"\"\n",
    "    all_ingredients = []\n",
    "    for i in xrange(len(x)):\n",
    "        x[i] = re.sub('\\([^)]+\\)\\s*', '', x[i]) #  remove brackets\n",
    "        x[i] = re.sub('[^\\\\%]+\\\\%\\s*', '', x[i]) #  remove percent signs\n",
    "        x[i] = x[i].strip().lower()  # lower case, trim\n",
    "        x[i] = stemmer.stem(x[i])  #  reduce ingredient to its stem, minor accuracy improvement\n",
    "        result = [x[i]] + x[i].split(' ')\n",
    "        for j in xrange(len(result)):\n",
    "            result[j] = result[j].strip().lower()\n",
    "            result[j] = re.sub('[^a-z]', ' ', result[j])\n",
    "        for ingredient in result:\n",
    "            if len(ingredient.strip()) != 0:\n",
    "                all_ingredients.append(ingredient)\n",
    "    return all_ingredients\n",
    "\n",
    "X_train_vectorizer = CountVectorizer(min_df=1, binary=True,\n",
    "                                     tokenizer=process_ingredients,\n",
    "                                     lowercase=False,\n",
    "                                     ngram_range=(1, 2),\n",
    "                                     dtype=np.uint8)\n",
    "\n",
    "# does significantly worse than just using counts\n",
    "#X_train_vectorizer = TfidfVectorizer(min_df=1, binary=True,\n",
    "#                                     tokenizer=process_ingredients,\n",
    "#                                     lowercase=False,\n",
    "#                                     ngram_range=(1, 2))\n",
    "\n",
    "# we have around 1 million features. you can use the hashing trick to\n",
    "# vectorize to a smaller number of features without losing too much\n",
    "# information. could be an interesting step to take before deep\n",
    "# neural network trainining (reduce the number of features to allow\n",
    "# larger and more complex models to fit in the GPU's memory). however\n",
    "# this is pointless for SGDClassifier (i.e. logistic regression) because\n",
    "# we can fit the counts as a sparse matrix in memory, and we don't mind\n",
    "# how long it takes to train the model.\n",
    "#X_train_vectorizer = HashingVectorizer(binary=True,\n",
    "#                                       tokenizer=process_ingredients,\n",
    "#                                       lowercase=False,\n",
    "#                                       ngram_range=(1, 2),\n",
    "#                                       n_features=500000)\n",
    "\n",
    "X_train_tf = X_train_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[7 9 ..., 1 7], n_iter=10, test_size=0.1, random_state=42),\n",
       "       error_score='raise',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,\n",
       "       penalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0,\n",
       "       warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['elasticnet'], 'alpha': [0.0001], 'n_iter': [5], 'loss': ['log'], 'l1_ratio': array([ 0.     ,  0.05556,  0.11111,  0.16667,  0.22222,  0.27778,\n",
       "        0.33333,  0.38889,  0.44444,  0.5    ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'alpha': [0.0001],  # regularization, penalizes large coefficients\n",
    "    'l1_ratio': np.linspace(0, 0.5, num=10),  # elastic net param. 0 is L2, 1 is L1\n",
    "    'n_iter': [5],\n",
    "    'penalty': ['elasticnet'],\n",
    "    'loss': ['log'],\n",
    "}\n",
    "model = SGDClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "cv = StratifiedShuffleSplit(y_train, n_iter=10, test_size=0.1,\n",
    "                            random_state=42)\n",
    "grid = GridSearchCV(model, param_grid=param_grid, cv=cv)\n",
    "grid.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001,\n",
       " 'l1_ratio': 0.1111111111111111,\n",
       " 'loss': 'log',\n",
       " 'n_iter': 5,\n",
       " 'penalty': 'elasticnet'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_tf = X_train_vectorizer.transform(X_test)\n",
    "y_test_pred = grid.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778280542986\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(y_test, y_test_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_filepath = \"/mnt/kaggle-whats-cooking/test.json\"\n",
    "submission = pd.read_json(submission_filepath)\n",
    "X_submission = submission[\"ingredients\"]\n",
    "X_submission_df = X_train_vectorizer.transform(X_submission)\n",
    "y_submission = grid.predict(X_submission_df)\n",
    "y_cuisine = cuisine_encoder.inverse_transform(y_submission)\n",
    "df = pd.DataFrame({\"cuisine\": y_cuisine}, index=submission[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See:\n",
    "\n",
    "- http://nbviewer.ipython.org/github/Egor-Krivov/cooking/blob/master/kaggle.ipynb\n",
    "- https://github.com/Egor-Krivov/cooking/blob/master/utils.py\n",
    "- https://github.com/fchollet/keras/blob/master/examples/kaggle_otto_nn.py\n",
    "- http://cs231n.github.io/\n",
    "\n",
    "Papers:\n",
    "\n",
    "- http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a way to check if you're successfully using the GPU, or falling back to the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GpuElemwise{exp,no_inplace}(<CudaNdarrayType(float32, vector)>), HostFromGpu(GpuElemwise{exp,no_inplace}.0)]\n",
      "Looping 1000 times took 0.663414 seconds\n",
      "Result is [ 1.23178029  1.61879349  1.52278066 ...,  2.20771813  2.29967761\n",
      "  1.62323296]\n",
      "Used the gpu\n"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, sandbox\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], T.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in xrange(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_hot_encoded = np.array(\n",
    "    np.zeros((y_train.shape[0], len(cuisine_encoder.classes_))),\n",
    "    dtype=np.int8)\n",
    "for i, value in enumerate(y_train):\n",
    "    y_train_hot_encoded[i, value] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compile the model and load it onto the GPU. This takes a few minutes. If you get a memory error restart the kernel and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dropout = 0.75\n",
    "dropout = 0.75\n",
    "l1 = 0\n",
    "l2 = 0\n",
    "layers = [1024, 1024]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dropout(input_dropout, input_shape=(X_train_tf.shape[1], )))  # regularize inputs\n",
    "for layer in layers:\n",
    "    model.add(Dense(layer, W_regularizer=l1l2(l1, l2)))\n",
    "\n",
    "    # choose an activation\n",
    "    model.add(Activation('relu'))  # the default activation to try first\n",
    "    #model.add(PReLU())\n",
    "\n",
    "    #model.add(BatchNormalization())  # makes it worse\n",
    "    model.add(Dropout(dropout))  # regularization, 0.5 is a good default dropout rate\n",
    "model.add(Dense(y_train_hot_encoded.shape[1], W_regularizer=l1l2(l1, l2)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# optimizers.\n",
    "# adam is much faster. i think too fast, need to slow down learning rate from default.\n",
    "# sgd is more optimal (i think). lr 0.1 seems a bit slow, but the default is 0.01!\n",
    "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = Adam()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "model_description = model.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we train. You can still abort the notebook here, but you'll probably have to restart the kernel and reload the data. Note that when you want to re-run this if you get memory errors you need to restart the kernel and reload the data.\n",
    "\n",
    "Also note that if you interrupt the kernel and restart training it doesn't start from scratch. Restarting the kernel will of course start everything from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28636 samples, validate on 3182 samples\n",
      "Epoch 1/100\n",
      "28636/28636 [==============================] - 26s - loss: 2.5530 - acc: 0.2628 - val_loss: 2.0230 - val_acc: 0.4730\n",
      "Epoch 2/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.9741 - acc: 0.4494 - val_loss: 1.5302 - val_acc: 0.5569\n",
      "Epoch 3/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.7101 - acc: 0.5069 - val_loss: 1.3413 - val_acc: 0.6072\n",
      "Epoch 4/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.5840 - acc: 0.5364 - val_loss: 1.2602 - val_acc: 0.6464\n",
      "Epoch 5/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.5131 - acc: 0.5539 - val_loss: 1.1952 - val_acc: 0.6615\n",
      "Epoch 6/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.4627 - acc: 0.5651 - val_loss: 1.1669 - val_acc: 0.6725\n",
      "Epoch 7/100\n",
      "28636/28636 [==============================] - 26s - loss: 1.4224 - acc: 0.5793 - val_loss: 1.1328 - val_acc: 0.6807\n",
      "Epoch 8/100\n",
      "28636/28636 [==============================] - 26s - loss: 1.3912 - acc: 0.5849 - val_loss: 1.1022 - val_acc: 0.6898\n",
      "Epoch 9/100\n",
      "28636/28636 [==============================] - 26s - loss: 1.3683 - acc: 0.5945 - val_loss: 1.0878 - val_acc: 0.7002\n",
      "Epoch 10/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.3339 - acc: 0.6023 - val_loss: 1.0662 - val_acc: 0.7071\n",
      "Epoch 11/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.3075 - acc: 0.6105 - val_loss: 1.0490 - val_acc: 0.7143\n",
      "Epoch 12/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.3023 - acc: 0.6133 - val_loss: 1.0399 - val_acc: 0.7187\n",
      "Epoch 13/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.2938 - acc: 0.6156 - val_loss: 1.0382 - val_acc: 0.7209\n",
      "Epoch 14/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.2642 - acc: 0.6193 - val_loss: 1.0235 - val_acc: 0.7244\n",
      "Epoch 15/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.2586 - acc: 0.6238 - val_loss: 1.0074 - val_acc: 0.7266\n",
      "Epoch 16/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.2349 - acc: 0.6315 - val_loss: 1.0001 - val_acc: 0.7275\n",
      "Epoch 17/100\n",
      "28636/28636 [==============================] - 26s - loss: 1.2208 - acc: 0.6340 - val_loss: 0.9940 - val_acc: 0.7300\n",
      "Epoch 18/100\n",
      "28636/28636 [==============================] - 25s - loss: 1.2170 - acc: 0.6367 - val_loss: 0.9849 - val_acc: 0.7363\n",
      "Epoch 19/100\n",
      "28636/28636 [==============================] - 25s - loss: 1.2151 - acc: 0.6357 - val_loss: 0.9822 - val_acc: 0.7370\n",
      "Epoch 20/100\n",
      "28636/28636 [==============================] - 26s - loss: 1.2018 - acc: 0.6385 - val_loss: 0.9698 - val_acc: 0.7392\n",
      "Epoch 21/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.1884 - acc: 0.6429 - val_loss: 0.9643 - val_acc: 0.7404\n",
      "Epoch 22/100\n",
      "28636/28636 [==============================] - 26s - loss: 1.1822 - acc: 0.6453 - val_loss: 0.9524 - val_acc: 0.7404\n",
      "Epoch 23/100\n",
      "28636/28636 [==============================] - 26s - loss: 1.1682 - acc: 0.6477 - val_loss: 0.9509 - val_acc: 0.7445\n",
      "Epoch 24/100\n",
      "28636/28636 [==============================] - 26s - loss: 1.1638 - acc: 0.6494 - val_loss: 0.9521 - val_acc: 0.7473\n",
      "Epoch 25/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.1559 - acc: 0.6521 - val_loss: 0.9389 - val_acc: 0.7483\n",
      "Epoch 26/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.1479 - acc: 0.6555 - val_loss: 0.9357 - val_acc: 0.7502\n",
      "Epoch 27/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.1329 - acc: 0.6572 - val_loss: 0.9263 - val_acc: 0.7511\n",
      "Epoch 28/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.1332 - acc: 0.6582 - val_loss: 0.9247 - val_acc: 0.7502\n",
      "Epoch 29/100\n",
      "28636/28636 [==============================] - 25s - loss: 1.1217 - acc: 0.6633 - val_loss: 0.9257 - val_acc: 0.7489\n",
      "Epoch 30/100\n",
      "28636/28636 [==============================] - 25s - loss: 1.1220 - acc: 0.6627 - val_loss: 0.9219 - val_acc: 0.7505\n",
      "Epoch 31/100\n",
      "28636/28636 [==============================] - 25s - loss: 1.1135 - acc: 0.6641 - val_loss: 0.9134 - val_acc: 0.7536\n",
      "Epoch 32/100\n",
      "28636/28636 [==============================] - 25s - loss: 1.1018 - acc: 0.6680 - val_loss: 0.9050 - val_acc: 0.7517\n",
      "Epoch 33/100\n",
      "28636/28636 [==============================] - 25s - loss: 1.0897 - acc: 0.6711 - val_loss: 0.8918 - val_acc: 0.7530\n",
      "Epoch 34/100\n",
      "28636/28636 [==============================] - 26s - loss: 1.0910 - acc: 0.6711 - val_loss: 0.8941 - val_acc: 0.7599\n",
      "Epoch 35/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.0904 - acc: 0.6692 - val_loss: 0.8923 - val_acc: 0.7599\n",
      "Epoch 36/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.0779 - acc: 0.6733 - val_loss: 0.8918 - val_acc: 0.7596\n",
      "Epoch 37/100\n",
      "28636/28636 [==============================] - 26s - loss: 1.0773 - acc: 0.6734 - val_loss: 0.8874 - val_acc: 0.7583\n",
      "Epoch 38/100\n",
      "28636/28636 [==============================] - 26s - loss: 1.0701 - acc: 0.6781 - val_loss: 0.8857 - val_acc: 0.7568\n",
      "Epoch 39/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.0665 - acc: 0.6793 - val_loss: 0.8737 - val_acc: 0.7608\n",
      "Epoch 40/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.0633 - acc: 0.6767 - val_loss: 0.8721 - val_acc: 0.7577\n",
      "Epoch 41/100\n",
      "28636/28636 [==============================] - 26s - loss: 1.0568 - acc: 0.6785 - val_loss: 0.8732 - val_acc: 0.7624\n",
      "Epoch 42/100\n",
      "28636/28636 [==============================] - 25s - loss: 1.0460 - acc: 0.6846 - val_loss: 0.8679 - val_acc: 0.7646\n",
      "Epoch 43/100\n",
      "28636/28636 [==============================] - 25s - loss: 1.0350 - acc: 0.6853 - val_loss: 0.8542 - val_acc: 0.7630\n",
      "Epoch 44/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.0297 - acc: 0.6877 - val_loss: 0.8517 - val_acc: 0.7659\n",
      "Epoch 45/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.0285 - acc: 0.6892 - val_loss: 0.8484 - val_acc: 0.7671\n",
      "Epoch 46/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.0439 - acc: 0.6853 - val_loss: 0.8492 - val_acc: 0.7656\n",
      "Epoch 47/100\n",
      "28636/28636 [==============================] - 26s - loss: 1.0164 - acc: 0.6871 - val_loss: 0.8446 - val_acc: 0.7665\n",
      "Epoch 48/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.0180 - acc: 0.6915 - val_loss: 0.8425 - val_acc: 0.7656\n",
      "Epoch 49/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.0095 - acc: 0.6939 - val_loss: 0.8363 - val_acc: 0.7703\n",
      "Epoch 50/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.0129 - acc: 0.6955 - val_loss: 0.8402 - val_acc: 0.7712\n",
      "Epoch 51/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.0054 - acc: 0.6947 - val_loss: 0.8314 - val_acc: 0.7690\n",
      "Epoch 52/100\n",
      "28636/28636 [==============================] - 27s - loss: 1.0040 - acc: 0.6943 - val_loss: 0.8264 - val_acc: 0.7687\n",
      "Epoch 53/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9953 - acc: 0.6979 - val_loss: 0.8286 - val_acc: 0.7715\n",
      "Epoch 54/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9887 - acc: 0.6994 - val_loss: 0.8242 - val_acc: 0.7715\n",
      "Epoch 55/100\n",
      "28636/28636 [==============================] - 26s - loss: 0.9855 - acc: 0.6991 - val_loss: 0.8201 - val_acc: 0.7725\n",
      "Epoch 56/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9684 - acc: 0.7087 - val_loss: 0.8115 - val_acc: 0.7753\n",
      "Epoch 57/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9704 - acc: 0.7070 - val_loss: 0.8147 - val_acc: 0.7766\n",
      "Epoch 58/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9723 - acc: 0.7032 - val_loss: 0.8135 - val_acc: 0.7734\n",
      "Epoch 59/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9705 - acc: 0.6999 - val_loss: 0.8080 - val_acc: 0.7709\n",
      "Epoch 60/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9644 - acc: 0.7069 - val_loss: 0.8062 - val_acc: 0.7762\n",
      "Epoch 61/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9818 - acc: 0.7001 - val_loss: 0.8149 - val_acc: 0.7715\n",
      "Epoch 62/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9666 - acc: 0.7052 - val_loss: 0.8086 - val_acc: 0.7750\n",
      "Epoch 63/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9637 - acc: 0.7070 - val_loss: 0.8030 - val_acc: 0.7753\n",
      "Epoch 64/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9523 - acc: 0.7094 - val_loss: 0.7982 - val_acc: 0.7769\n",
      "Epoch 65/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9469 - acc: 0.7106 - val_loss: 0.7913 - val_acc: 0.7747\n",
      "Epoch 66/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9470 - acc: 0.7079 - val_loss: 0.7938 - val_acc: 0.7744\n",
      "Epoch 67/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9381 - acc: 0.7142 - val_loss: 0.7882 - val_acc: 0.7734\n",
      "Epoch 68/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9334 - acc: 0.7143 - val_loss: 0.7895 - val_acc: 0.7753\n",
      "Epoch 69/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9339 - acc: 0.7140 - val_loss: 0.7891 - val_acc: 0.7731\n",
      "Epoch 70/100\n",
      "28636/28636 [==============================] - 27s - loss: 0.9358 - acc: 0.7132 - val_loss: 0.7906 - val_acc: 0.7737\n",
      "Epoch 71/100\n",
      "28636/28636 [==============================] - 26s - loss: 0.9370 - acc: 0.7135 - val_loss: 0.7903 - val_acc: 0.7750\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 100\n",
    "\n",
    "# make this as large as possible without running out of memory\n",
    "# (i think! you should play around with this and see what your results are)\n",
    "# on kaggle people seem to use 16-128, but to get fast results use 1024+. 4096 runs out of memory.\n",
    "batch_size = 2048\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_tf.todense(),\n",
    "    y_train_hot_encoded,\n",
    "    nb_epoch=nb_epoch,\n",
    "    batch_size=batch_size,\n",
    "    show_accuracy=True,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw history of accuracies during training to see if we're overfitting or underfitting. In keras plotting loss is very misleading because we train on the regularized model using dropout, but we test without dropout. Hence validation loss is always smaller than training loss, and hard to interpret.\n",
    "\n",
    "Then again I've noticed that validation accuracy is usually higher than train accuracy, which doesn't make sense..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EpochDrawer(object):\n",
    "    '''\n",
    "    takes the history of keras.models.Sequential().fit() and\n",
    "    plots training and validation accuracy over the epochs\n",
    "    '''\n",
    "    def __init__(self, history, key='acc', save_filename = None):\n",
    "\n",
    "        self.x = history.epoch\n",
    "        self.legend = [key]\n",
    "\n",
    "        plt.plot(self.x, history.history[key], marker='.')\n",
    "\n",
    "        if 'val_%s' % key in history.history:\n",
    "            self.legend.append('val %s' % key)\n",
    "            plt.plot(self.x, history.history['val_%s' % key], marker='.')\n",
    "\n",
    "        plt.title('%s over epochs' % key)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.xticks(history.epoch, history.epoch)\n",
    "        plt.legend(self.legend, loc = 'lower right')\n",
    "\n",
    "        if save_filename is not None:\n",
    "            plt.savefig(save_filename)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEZCAYAAABsPmXUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvLC4Ru3HAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb1b4a87f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EpochDrawer(history)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember - the smaller the gap between the curves the higher the bias. Small gap => try more complex, big gap => you need to regularize more.\n",
    "\n",
    "In this case I don't really know if we're high bias or high variance. Needs more experimentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7956/7956 [==============================] - 5s     \n"
     ]
    }
   ],
   "source": [
    "X_test_tf = X_train_vectorizer.transform(X_test)\n",
    "y_test_pred = model.predict_classes(X_test_tf.todense(), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79009552539\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(y_test, y_test_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9944/9944 [==============================] - 6s     \n"
     ]
    }
   ],
   "source": [
    "submission_filepath = \"/mnt/kaggle-whats-cooking/test.json\"\n",
    "submission = pd.read_json(submission_filepath)\n",
    "X_submission = submission[\"ingredients\"]\n",
    "X_submission_df = X_train_vectorizer.transform(X_submission)\n",
    "y_submission = model.predict_classes(X_submission_df.todense(), batch_size=batch_size)\n",
    "y_cuisine = cuisine_encoder.inverse_transform(y_submission)\n",
    "df = pd.DataFrame({\"cuisine\": y_cuisine}, index=submission[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"submission_nn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO - ensemble NN and logistic regression?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
